{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asgar\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from itertools import product\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "data = data.drop(\n",
    "    [\n",
    "        \"timestamp\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data[\"is_anomaly\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def normalize_data(data):\n",
    "    scaling_rules = {\n",
    "        \"cpu_temperature\": (25.0, 100.0),\n",
    "        \"gpu_temperature\": (25.0, 100.0),\n",
    "        \"cpu_speed\": (0.60, 4.80),\n",
    "        \"cpu_fan_speed\": (0.0, 7500.0),\n",
    "        \"gpu_fan_speed\": (0.0, 7500.0),\n",
    "        \"disk_reads\": (0.0, 3000.0),\n",
    "        \"disk_writes\": (0.0, 3000.0),\n",
    "        \"network_bytes_sent\": (0.0, 200000.0),\n",
    "        \"network_bytes_received\": (0.0, 5000000.0),\n",
    "        \"network_packets_sent\": (0.0, 500.0),\n",
    "        \"network_packets_received\": (0.0, 4000.0),\n",
    "    }\n",
    "\n",
    "    for column, (min_val, max_val) in scaling_rules.items():\n",
    "        if column in data.columns:\n",
    "            if data[column].min() >= 0 and data[column].max() <= 1:\n",
    "                continue\n",
    "\n",
    "            data[column] = (data[column] - min_val) / (max_val - min_val)\n",
    "            data[column] = data[column].clip(0, 1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def synthesize_anomaly(data_row, scenario=None):\n",
    "    data_row = data_row.copy()\n",
    "\n",
    "    data_row[\"is_anomaly\"] = 1\n",
    "\n",
    "    if scenario == None:\n",
    "        scenario = np.random.choice(\n",
    "            [\n",
    "                \"thermal_overload\",\n",
    "                \"cpu_overload\",\n",
    "                \"disk_filesystem_abuse\",\n",
    "                \"network_abuse\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    if scenario == \"thermal_overload\":\n",
    "        data_row[\"cpu_temperature\"] += 15\n",
    "        data_row[\"cpu_temperature\"] = min(data_row[\"cpu_temperature\"], 100)\n",
    "\n",
    "        data_row[\"gpu_temperature\"] += 15\n",
    "        data_row[\"gpu_temperature\"] = min(data_row[\"gpu_temperature\"], 100)\n",
    "\n",
    "        data_row[\"cpu_fan_speed\"] += 1000\n",
    "        data_row[\"gpu_fan_speed\"] += 1000\n",
    "\n",
    "        data_row[\"cpu_fan_speed\"] = min(data_row[\"cpu_fan_speed\"], 7500)\n",
    "        data_row[\"gpu_fan_speed\"] = min(data_row[\"gpu_fan_speed\"], 7500)\n",
    "\n",
    "    elif scenario == \"disk_filesystem_abuse\":\n",
    "        data_row[\"cpu_usage\"] *= 1.20\n",
    "        data_row[\"cpu_speed\"] *= 1.30\n",
    "\n",
    "        data_row[\"disk_reads\"] += 500\n",
    "        data_row[\"disk_writes\"] += 500\n",
    "\n",
    "        data_row[\"disk_reads\"] = min(data_row[\"disk_reads\"], 3000)\n",
    "        data_row[\"disk_writes\"] = min(data_row[\"disk_writes\"], 3000)\n",
    "\n",
    "    elif scenario == \"network_abuse\":\n",
    "        data_row[\"cpu_usage\"] *= 1.20\n",
    "        data_row[\"cpu_speed\"] *= 1.30\n",
    "\n",
    "        data_row[\"network_bytes_sent\"] += 10000\n",
    "        data_row[\"network_bytes_received\"] += 250000\n",
    "\n",
    "        data_row[\"network_packets_sent\"] += 120\n",
    "        data_row[\"network_packets_received\"] += 600\n",
    "\n",
    "        data_row[\"cpu_usage\"] = min(data_row[\"cpu_usage\"], 1.0)\n",
    "        data_row[\"cpu_speed\"] = min(data_row[\"cpu_speed\"], 4.80)\n",
    "\n",
    "        data_row[\"network_bytes_sent\"] = min(data_row[\"network_bytes_sent\"], 200000)\n",
    "        data_row[\"network_bytes_received\"] = min(\n",
    "            data_row[\"network_bytes_received\"], 5000000\n",
    "        )\n",
    "\n",
    "        data_row[\"network_packets_sent\"] = min(data_row[\"network_packets_sent\"], 500)\n",
    "        data_row[\"network_packets_received\"] = min(\n",
    "            data_row[\"network_packets_received\"], 4000\n",
    "        )\n",
    "\n",
    "    elif scenario == \"cpu_overload\":\n",
    "        data_row[\"cpu_usage\"] *= 1.20\n",
    "        data_row[\"cpu_speed\"] *= 1.30\n",
    "        data_row[\"cpu_fan_speed\"] += 1000\n",
    "\n",
    "        data_row[\"cpu_usage\"] = min(data_row[\"cpu_usage\"], 1.0)\n",
    "        data_row[\"cpu_speed\"] = min(data_row[\"cpu_speed\"], 4.80)\n",
    "        data_row[\"cpu_fan_speed\"] = min(data_row[\"cpu_fan_speed\"], 7500)\n",
    "\n",
    "    return data_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1492\n",
      "Learning period rows: 720\n",
      "Retraining period rows: 360.0\n",
      "Total anomaly rows: 80\n"
     ]
    }
   ],
   "source": [
    "learning_period_seconds = 60 * 60 * 1  # 1 hour\n",
    "retraining_period_seconds = 60 * 60 * 0.5  # 30 minutes\n",
    "\n",
    "# Each row is collected every 5 seconds\n",
    "LEARNING_PERIOD_ROWS_COUNT = learning_period_seconds // 5\n",
    "RETRAINING_PERIOD_ROWS_COUNT = retraining_period_seconds // 5\n",
    "TOTAL_ROWS_COUNT = len(data)\n",
    "random_state = 42\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "\n",
    "# Generate Synthetic Anomalies\n",
    "i = LEARNING_PERIOD_ROWS_COUNT\n",
    "while i < TOTAL_ROWS_COUNT:\n",
    "    rand_number = np.random.randint(0, 2)\n",
    "\n",
    "    if rand_number == 1:\n",
    "        anomaly_count = np.random.randint(3, 7)\n",
    "        for j in range(i, i + anomaly_count):\n",
    "            if j < TOTAL_ROWS_COUNT:\n",
    "                data.loc[j] = synthesize_anomaly(data.iloc[j])\n",
    "        i += anomaly_count\n",
    "\n",
    "    i += 15\n",
    "\n",
    "TOTAL_ANOMALY_ROWS_COUNT = data[\"is_anomaly\"].sum()\n",
    "\n",
    "print(f\"Total rows: {TOTAL_ROWS_COUNT}\")\n",
    "print(f\"Learning period rows: {LEARNING_PERIOD_ROWS_COUNT}\")\n",
    "print(f\"Retraining period rows: {RETRAINING_PERIOD_ROWS_COUNT}\")\n",
    "print(f\"Total anomaly rows: {TOTAL_ANOMALY_ROWS_COUNT}\")\n",
    "\n",
    "# Normalize data\n",
    "data = normalize_data(data)\n",
    "data_labels = data[\"is_anomaly\"]\n",
    "data = data.drop([\"is_anomaly\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OneClassSVMWrapper:\n",
    "    def __init__(self, kernel=\"rbf\", nu=0.05, gamma=\"scale\", features=[]):\n",
    "        self.model = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X[self.features]\n",
    "        self.model.fit(self.X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        # One-Class SVM returns +1 for inliers, -1 for outliers.\n",
    "        # We map outliers (-1) to anomaly (1) and inliers (+1) to normal (0).\n",
    "        preds = self.model.predict(self.X)\n",
    "        return [1 if p == -1 else 0 for p in preds]\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"OneClassSVM\"\n",
    "\n",
    "\n",
    "class LOFWrapper:\n",
    "    def __init__(self, n_neighbors=20, novelty=True, contamination=0.05, features=[]):\n",
    "        # novelty=True allows LOF to be used for out-of-sample predictions.\n",
    "        self.model = LocalOutlierFactor(\n",
    "            n_neighbors=n_neighbors, novelty=novelty, contamination=contamination\n",
    "        )\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X[self.features]\n",
    "        self.model.fit(self.X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        # LOF returns 1 for inliers, -1 for outliers.\n",
    "        preds = self.model.predict(self.X)\n",
    "        return [1 if p == -1 else 0 for p in preds]\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"LocalOutlierFactor\"\n",
    "\n",
    "\n",
    "class EllipticEnvelopeWrapper:\n",
    "    def __init__(self, contamination=0.1, support_fraction=0.8, features=[]):\n",
    "        self.model = EllipticEnvelope(\n",
    "            contamination=contamination,\n",
    "            support_fraction=support_fraction,\n",
    "            random_state=42,\n",
    "        )\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X[self.features]\n",
    "        self.model.fit(self.X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        # EllipticEnvelope returns +1 for inliers and -1 for outliers.\n",
    "        preds = self.model.predict(self.X)\n",
    "        return [1 if p == -1 else 0 for p in preds]\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"EllipticEnvelopeWrapper\"\n",
    "\n",
    "\n",
    "class GaussianMixtureWrapper:\n",
    "    def __init__(\n",
    "        self, n_components=1, covariance_type=\"full\", threshold=None, features=[]\n",
    "    ):\n",
    "        self.features = features\n",
    "        self.model = GaussianMixture(\n",
    "            n_components=n_components, covariance_type=covariance_type\n",
    "        )\n",
    "        self.threshold = threshold \n",
    "        self.fitted_scores = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X[self.features]\n",
    "        self.model.fit(self.X)\n",
    "        # Compute log-likelihood for training data.\n",
    "        scores = self.model.score_samples(self.X)\n",
    "        self.fitted_scores = scores\n",
    "        if self.threshold is None:\n",
    "            # For example, set threshold as (mean - std) of the training scores.\n",
    "            self.threshold = scores.mean() - scores.std()\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        # Compute log-likelihood for each sample.\n",
    "        scores = self.model.score_samples(self.X)\n",
    "        # Label as anomaly (1) if the score is below the threshold; normal (0) otherwise.\n",
    "        return [1 if score < self.threshold else 0 for score in scores]\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"GaussianMixture\"\n",
    "\n",
    "\n",
    "class IsolationForestWrapper:\n",
    "    def __init__(self, n_estimators=100, contamination=0.05, features=[]):\n",
    "        self.model = IsolationForest(\n",
    "            n_estimators=n_estimators, contamination=contamination\n",
    "        )\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X[self.features]\n",
    "        self.model.fit(self.X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        # Isolation Forest returns 1 for inliers, -1 for outliers.\n",
    "        # We map outliers (-1) to anomaly (1) and inliers (+1) to normal (0).\n",
    "        preds = self.model.predict(self.X)\n",
    "        return [1 if p == -1 else 0 for p in preds]\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"IsolationForest\"\n",
    "\n",
    "\n",
    "class MaxRuleBasedWrapper:\n",
    "    critical_values = {}\n",
    "\n",
    "    def __init__(self, n=2, percentage=0.10, features=[]):\n",
    "        self.n = n\n",
    "        self.percentage = percentage\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = X[self.features]\n",
    "        features = X.columns\n",
    "\n",
    "        for feature in features:\n",
    "            min_val = X[feature].min()\n",
    "            max_val = X[feature].max()\n",
    "            self.critical_values[feature] = (min_val, max_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        preds = []\n",
    "\n",
    "        for i in range(len(self.X)):\n",
    "            row = self.X.iloc[i]\n",
    "            outlier_count = 0\n",
    "\n",
    "            for feature, (min_val, max_val) in self.critical_values.items():\n",
    "                if row[feature] > max_val * (1 + self.percentage):\n",
    "                    outlier_count += 1\n",
    "\n",
    "            if outlier_count >= self.n:\n",
    "                preds.append(1)\n",
    "            else:\n",
    "                preds.append(0)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"MaxRuleBasedWrapper\"\n",
    "\n",
    "\n",
    "class AverageRuleBasedWrapper:\n",
    "    average_values = {}\n",
    "\n",
    "    def __init__(self, n=2, percentage=0.10, features=[]):\n",
    "        self.n = n\n",
    "        self.percentage = percentage\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X[self.features]\n",
    "\n",
    "        for feature in self.features:\n",
    "            mean_val = X[feature].mean()\n",
    "            self.average_values[feature] = mean_val\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        preds = []\n",
    "\n",
    "        for i in range(len(self.X)):\n",
    "            row = self.X.iloc[i]\n",
    "            outlier_count = 0\n",
    "\n",
    "            for feature, mean_val in self.average_values.items():\n",
    "                if row[feature] > mean_val * (1 + self.percentage):\n",
    "                    outlier_count += 1\n",
    "\n",
    "            if outlier_count >= self.n:\n",
    "                preds.append(1)\n",
    "            else:\n",
    "                preds.append(0)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"AverageRuleBasedWrapper\"\n",
    "\n",
    "\n",
    "class ZScoreWrapper:\n",
    "    z_scores = {}\n",
    "\n",
    "    def __init__(self, n=2, threshold=3.0, features=[]):\n",
    "        self.n = n\n",
    "        self.threshold = threshold\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X[self.features]\n",
    "\n",
    "        for feature in self.features:\n",
    "            mean_val = self.X[feature].mean()\n",
    "            std_val = self.X[feature].std()\n",
    "            self.z_scores[feature] = (mean_val, std_val)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X = X[self.features]\n",
    "        preds = []\n",
    "\n",
    "        for i in range(len(self.X)):\n",
    "            row = self.X.iloc[i]\n",
    "            outlier_count = 0\n",
    "\n",
    "            for feature, (mean_val, std_val) in self.z_scores.items():\n",
    "                z_score = (row[feature] - mean_val) / std_val\n",
    "                if z_score > self.threshold:\n",
    "                    outlier_count += 1\n",
    "\n",
    "            if outlier_count >= self.n:\n",
    "                preds.append(1)\n",
    "            else:\n",
    "                preds.append(0)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"ZScoreWrapper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods(data, data_labels, methods):\n",
    "    \"\"\"\n",
    "    Evaluate a set of anomaly detection methods with various hyperparameter combinations.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Preprocessed telemetry data without labels.\n",
    "        data_labels (Series): The true anomaly labels corresponding to the data (binary values).\n",
    "        methods (dict): A dictionary where each key is a method name and the value is a dictionary with:\n",
    "            - 'func': The detector constructor/function (which returns an instance with fit and predict methods)\n",
    "            - 'hyperparams': (Optional) A dictionary of hyperparameter names mapped to lists of possible values.\n",
    "\n",
    "    Returns:\n",
    "        results (list of dict): A list of result dictionaries, each containing:\n",
    "            - 'method': The method name.\n",
    "            - 'params': The hyperparameter configuration.\n",
    "            - 'accuracy', 'recall', 'precision', 'f1': Performance metrics.\n",
    "            - 'confusion_matrix': The confusion matrix.\n",
    "        Only the top 10 configurations (by F1 score) for each method are returned.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Assume these global variables are defined\n",
    "    global LEARNING_PERIOD_ROWS_COUNT, RETRAINING_PERIOD_ROWS_COUNT, TOTAL_ROWS_COUNT\n",
    "\n",
    "    for method_name, method_config in methods.items():\n",
    "        method_func = method_config[\"func\"]\n",
    "        hyperparams = method_config.get(\"hyperparams\", {})\n",
    "        features = method_config.get(\"features\", [])\n",
    "\n",
    "        # Generate all hyperparameter combinations (if hyperparams are provided)\n",
    "        if hyperparams:\n",
    "            param_names, param_values = zip(*hyperparams.items())\n",
    "            hyperparam_combinations = [\n",
    "                dict(zip(param_names, values)) for values in product(*param_values)\n",
    "            ]\n",
    "        else:\n",
    "            hyperparam_combinations = [{}]\n",
    "\n",
    "        method_results = []\n",
    "\n",
    "        for params in hyperparam_combinations:\n",
    "            # Instantiate the detector using the method constructor and current hyperparameters.\n",
    "            detector_instance = method_func(features=features, **params)\n",
    "\n",
    "            # Start training on the learning period\n",
    "            i = LEARNING_PERIOD_ROWS_COUNT\n",
    "            detector_instance.fit(data.iloc[:i])\n",
    "            predicted_labels = []\n",
    "\n",
    "            training_data = data.iloc[:i].copy()\n",
    "\n",
    "            # Iterate over remaining rows (simulate real-time prediction)\n",
    "            while i < TOTAL_ROWS_COUNT:\n",
    "                # Retrain periodically (after each retraining period)\n",
    "                if (\n",
    "                    i - LEARNING_PERIOD_ROWS_COUNT + 1\n",
    "                ) % RETRAINING_PERIOD_ROWS_COUNT == 0:\n",
    "                    detector_instance.fit(training_data)\n",
    "\n",
    "                # Predict anomaly for the current row (assumed to return a list/array)\n",
    "                anomaly_flags = detector_instance.predict(data.iloc[i : i + 1])\n",
    "                predicted_labels.append(anomaly_flags[0])\n",
    "                is_anomaly = anomaly_flags[0] == 1\n",
    "\n",
    "                if not is_anomaly:\n",
    "                    training_data = training_data.iloc[1:]\n",
    "                    training_data = pd.concat(\n",
    "                        [training_data, data.iloc[[i]]], ignore_index=True\n",
    "                    )\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # Evaluate performance on the test portion (rows after the learning period)\n",
    "            y_true = data_labels[LEARNING_PERIOD_ROWS_COUNT:].tolist()\n",
    "            y_pred = predicted_labels\n",
    "\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "            result = {\n",
    "                \"method\": method_name,\n",
    "                \"params\": params,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"recall\": recall,\n",
    "                \"precision\": precision,\n",
    "                \"f1\": f1,\n",
    "                \"confusion_matrix\": conf_matrix,\n",
    "            }\n",
    "            method_results.append(result)\n",
    "\n",
    "        # Sort results for the current method by F1 score (descending) and keep top 3.\n",
    "        method_results.sort(key=lambda r: r[\"f1\"], reverse=True)\n",
    "        top_results = method_results[:3]\n",
    "        results.extend(top_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "methods = {\n",
    "    'IsolationForest': {\n",
    "        'func': IsolationForestWrapper,\n",
    "        'hyperparams': {\n",
    "            'n_estimators': [50, 100, 150],\n",
    "            'contamination': [0.01, 0.05, 0.1, 0.2],\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            'cpu_fan_speed',\n",
    "        ],\n",
    "    },\n",
    "    'OneClassSVM': {\n",
    "        'func': OneClassSVMWrapper,\n",
    "        'hyperparams': {\n",
    "            'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "            'nu': [0.001, 0.01, 0.05, 0.1, 0.15],\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            'cpu_fan_speed',\n",
    "            'gpu_usage',\n",
    "            'gpu_temperature',\n",
    "            'gpu_fan_speed',\n",
    "            'disk_reads',\n",
    "            'disk_writes',\n",
    "            'network_bytes_sent',\n",
    "            'network_bytes_received',\n",
    "            'network_packets_sent',\n",
    "            'network_packets_received',\n",
    "        ],\n",
    "    },\n",
    "    'LocalOutlierFactor': {\n",
    "        'func': LOFWrapper,\n",
    "        'hyperparams': {\n",
    "            'n_neighbors': [10, 15, 20, 25, 30],\n",
    "            'novelty': [True],\n",
    "            'contamination': [0.01, 0.05, 0.1, 0.2],\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            'cpu_fan_speed',\n",
    "            'gpu_usage',\n",
    "            'gpu_temperature',\n",
    "            'gpu_fan_speed',\n",
    "            'disk_reads',\n",
    "            'disk_writes',\n",
    "            'network_bytes_sent',\n",
    "            'network_bytes_received',\n",
    "            'network_packets_sent',\n",
    "            'network_packets_received',\n",
    "        ],\n",
    "    },\n",
    "    'GaussianMixture': {\n",
    "        'func': GaussianMixtureWrapper,\n",
    "        'hyperparams': {\n",
    "            'n_components': [1, 2, 3],\n",
    "            'covariance_type': ['full', 'diag', 'spherical', 'tied'],\n",
    "            'threshold': [-3.0, -0.5, -2.0, -2.5, -3.5, -4]  # None means automatic thresholding based on training scores.\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            'cpu_fan_speed',\n",
    "            'gpu_usage',\n",
    "            'gpu_temperature',\n",
    "            'gpu_fan_speed',\n",
    "            'disk_reads',\n",
    "            'disk_writes',\n",
    "            'network_bytes_sent',\n",
    "            'network_bytes_received',\n",
    "            'network_packets_sent',\n",
    "            'network_packets_received',\n",
    "        ]\n",
    "    },\n",
    "    'MaxRuleBased': {\n",
    "        'func': MaxRuleBasedWrapper,\n",
    "        'hyperparams': {\n",
    "            'n': [2, 3, 4],\n",
    "            'percentage': [0.05, 0.10, 0.15, 0.20],\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            'cpu_fan_speed',\n",
    "            'gpu_usage',\n",
    "            'gpu_temperature',\n",
    "            'gpu_fan_speed',\n",
    "        ],\n",
    "    },\n",
    "    'AverageRuleBased': {\n",
    "        'func': AverageRuleBasedWrapper,\n",
    "        'hyperparams': {\n",
    "            'n': [2, 3, 4],\n",
    "            'percentage': [0.05, 0.10, 0.15, 0.20],\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            'cpu_fan_speed',\n",
    "        ],\n",
    "    },\n",
    "    'ZScore': {\n",
    "        'func': ZScoreWrapper,\n",
    "        'hyperparams': {\n",
    "            'n': [1, 2, 3, 4],\n",
    "            'threshold': [1.5, 2.5, 3.0, 3.5],\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            'cpu_fan_speed',\n",
    "            'gpu_usage',\n",
    "            'gpu_temperature',\n",
    "            'gpu_fan_speed',\n",
    "            'disk_reads',\n",
    "            'disk_writes',\n",
    "            'network_bytes_sent',\n",
    "            'network_bytes_received',\n",
    "            'network_packets_sent',\n",
    "            'network_packets_received',\n",
    "        ]\n",
    "    },\n",
    "    'EllipticEnvelopeWrapper': {\n",
    "        'func': EllipticEnvelopeWrapper,\n",
    "        'hyperparams': {\n",
    "            'contamination': [0.01, 0.05, 0.1, 0.2],\n",
    "            'support_fraction': [0.5, 0.6, 0.8, 0.9],\n",
    "        },\n",
    "        'features': [\n",
    "            'cpu_usage',\n",
    "            # 'cpu_temperature',\n",
    "            'cpu_speed',\n",
    "            # 'cpu_fan_speed',\n",
    "            # 'gpu_usage',\n",
    "            # 'gpu_temperature',\n",
    "            # 'gpu_fan_speed',\n",
    "            'disk_reads',\n",
    "            'disk_writes',\n",
    "            'network_bytes_sent',\n",
    "            'network_bytes_received',\n",
    "            'network_packets_sent',\n",
    "            'network_packets_received',\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "results = evaluate_methods(data, data_labels, methods)\n",
    "# Print top results for each method.\n",
    "for res in results:\n",
    "    print(\"Method:\", res['method'])\n",
    "    print(\"Hyperparameters:\", res['params'])\n",
    "    print(\"Accuracy: {:.3f}, Recall: {:.3f}, Precision: {:.3f}, F1: {:.3f}\".format(\n",
    "        res['accuracy'], res['recall'], res['precision'], res['f1']))\n",
    "    print(\"Confusion Matrix:\\n\", res['confusion_matrix'])\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1\n",
      "Accuracy: 0.918, Recall: 0.950, Precision: 0.563, F1: 0.707\n",
      "Confusion Matrix:\n",
      "[[633  59]\n",
      " [  4  76]]\n",
      "---------------------------------------------------\n",
      "Threshold: 2\n",
      "Accuracy: 0.978, Recall: 0.912, Precision: 0.880, F1: 0.896\n",
      "Confusion Matrix:\n",
      "[[682  10]\n",
      " [  7  73]]\n",
      "---------------------------------------------------\n",
      "Threshold: 3\n",
      "Accuracy: 0.969, Recall: 0.713, Precision: 0.983, F1: 0.826\n",
      "Confusion Matrix:\n",
      "[[691   1]\n",
      " [ 23  57]]\n",
      "---------------------------------------------------\n",
      "Threshold: 4\n",
      "Accuracy: 0.902, Recall: 0.050, Precision: 1.000, F1: 0.095\n",
      "Confusion Matrix:\n",
      "[[692   0]\n",
      " [ 76   4]]\n",
      "---------------------------------------------------\n",
      "Threshold: 5\n",
      "Accuracy: 0.898, Recall: 0.013, Precision: 1.000, F1: 0.025\n",
      "Confusion Matrix:\n",
      "[[692   0]\n",
      " [ 79   1]]\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def evaluate_final_detector(data, data_labels, final_detectors, threshold_range):\n",
    "    \"\"\"\n",
    "    Evaluate the final anomaly detection decision by aggregating predictions from multiple detectors.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Preprocessed telemetry data (without labels).\n",
    "        data_labels (Series): Ground truth binary anomaly labels (0 for normal, 1 for anomaly).\n",
    "        final_detectors (list): A list of pre-configured detector instances (each with fit() and predict() methods).\n",
    "        threshold_range (list of int): A list of thresholds (number of detectors required to flag an anomaly) to try.\n",
    "\n",
    "    Returns:\n",
    "        results (list of dict): A list of dictionaries, each containing:\n",
    "            - 'threshold': the aggregation threshold used,\n",
    "            - 'accuracy', 'recall', 'precision', 'f1': the computed performance metrics,\n",
    "            - 'confusion_matrix': the confusion matrix.\n",
    "    \"\"\"\n",
    "    # Global variables defined elsewhere:\n",
    "    # LEARNING_PERIOD_ROWS_COUNT, RETRAINING_PERIOD_ROWS_COUNT, TOTAL_ROWS_COUNT\n",
    "    # For this function, we assume that the detectors are already initialised with final hyperparameters.\n",
    "\n",
    "    # Create a copy of the initial training data for sliding-window retraining.\n",
    "    training_data = data.iloc[:LEARNING_PERIOD_ROWS_COUNT].copy()\n",
    "\n",
    "    # This list will hold, for every test data point, a list of predictions from all detectors.\n",
    "    aggregated_predictions = []\n",
    "\n",
    "    # Fit all detectors on the initial training data.\n",
    "    for detector in final_detectors:\n",
    "        detector.fit(training_data)\n",
    "\n",
    "    # Simulate real-time processing for test data (data points after the learning period).\n",
    "    for i in range(LEARNING_PERIOD_ROWS_COUNT, TOTAL_ROWS_COUNT):\n",
    "        # Retrain detectors periodically using the sliding window training data.\n",
    "        if (i - LEARNING_PERIOD_ROWS_COUNT + 1) % RETRAINING_PERIOD_ROWS_COUNT == 0:\n",
    "            for detector in final_detectors:\n",
    "                detector.fit(training_data)\n",
    "\n",
    "        # For the current data point, collect predictions from each detector.\n",
    "        current_preds = []\n",
    "        for detector in final_detectors:\n",
    "            # Assume each detector.predict returns a list/array; extract the first element.\n",
    "            pred = detector.predict(data.iloc[i : i + 1])\n",
    "            current_preds.append(pred[0])\n",
    "        aggregated_predictions.append(current_preds)\n",
    "\n",
    "        # Update training_data with current data point if all detectors agree it is normal.\n",
    "        if all(pred == 0 for pred in current_preds):\n",
    "            training_data = pd.concat(\n",
    "                [training_data, data.iloc[[i]]], ignore_index=True\n",
    "            )\n",
    "        # Else, if an anomaly is detected, the current point is not added to the training data.\n",
    "\n",
    "    # Extract ground truth for test period.\n",
    "    y_true = data_labels.iloc[LEARNING_PERIOD_ROWS_COUNT:].tolist()\n",
    "\n",
    "    # Evaluate final decision for each threshold.\n",
    "    results = []\n",
    "    for thresh in threshold_range:\n",
    "        # For each test data point, classify as anomaly if number of detectors predicting anomaly >= threshold.\n",
    "        y_pred_final = [\n",
    "            1 if sum(preds) >= thresh else 0 for preds in aggregated_predictions\n",
    "        ]\n",
    "\n",
    "        # Calculate performance metrics.\n",
    "        f1 = f1_score(y_true, y_pred_final)\n",
    "        accuracy = accuracy_score(y_true, y_pred_final)\n",
    "        recall = recall_score(y_true, y_pred_final)\n",
    "        precision = precision_score(y_true, y_pred_final)\n",
    "        conf_matrix = confusion_matrix(y_true, y_pred_final)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"threshold\": thresh,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"recall\": recall,\n",
    "                \"precision\": precision,\n",
    "                \"f1\": f1,\n",
    "                \"confusion_matrix\": conf_matrix,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "final_detectors = []\n",
    "\n",
    "detector_1 = GaussianMixtureWrapper(\n",
    "    n_components=3,\n",
    "    covariance_type=\"full\",\n",
    "    threshold=-2.0,\n",
    "    features=[\n",
    "        \"cpu_usage\",\n",
    "        \"cpu_speed\",\n",
    "        \"disk_reads\",\n",
    "        \"disk_writes\",\n",
    "        \"network_bytes_sent\",\n",
    "        \"network_bytes_received\",\n",
    "        \"network_packets_sent\",\n",
    "        \"network_packets_received\",\n",
    "    ],\n",
    ")\n",
    "detector_2 = AverageRuleBasedWrapper(\n",
    "    n=4,\n",
    "    percentage=0.20,\n",
    "    features=[\"cpu_usage\", \"cpu_temperature\", \"cpu_speed\", \"cpu_fan_speed\"],\n",
    ")\n",
    "detector_3 = OneClassSVMWrapper(\n",
    "    kernel=\"rbf\",\n",
    "    nu=0.001,\n",
    "    features=[\n",
    "        \"cpu_usage\",\n",
    "        \"cpu_temperature\",\n",
    "        \"cpu_speed\",\n",
    "        \"cpu_fan_speed\",\n",
    "        \"gpu_usage\",\n",
    "        \"gpu_temperature\",\n",
    "        \"gpu_fan_speed\",\n",
    "    ],\n",
    ")\n",
    "detector_4 = ZScoreWrapper(\n",
    "    n=3,\n",
    "    threshold=1.5,\n",
    "    features=[\n",
    "        \"cpu_usage\",\n",
    "        \"cpu_speed\",\n",
    "        \"disk_reads\",\n",
    "        \"disk_writes\",\n",
    "        \"network_bytes_sent\",\n",
    "        \"network_bytes_received\",\n",
    "        \"network_packets_sent\",\n",
    "        \"network_packets_received\",\n",
    "    ],\n",
    ")\n",
    "detector_5 = MaxRuleBasedWrapper(\n",
    "    n=2,\n",
    "    percentage=0.05,\n",
    "    features=[\n",
    "        \"cpu_usage\",\n",
    "        \"cpu_temperature\",\n",
    "        \"cpu_speed\",\n",
    "        \"cpu_fan_speed\",\n",
    "        \"gpu_usage\",\n",
    "        \"gpu_temperature\",\n",
    "        \"gpu_fan_speed\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "final_detectors = [detector_1, detector_2, detector_3, detector_4, detector_5]\n",
    "\n",
    "threshold_range = list(range(1, len(final_detectors) + 1))\n",
    "\n",
    "final_results = evaluate_final_detector(\n",
    "    data, data_labels, final_detectors, threshold_range\n",
    ")\n",
    "\n",
    "for res in final_results:\n",
    "    print(f\"Threshold: {res['threshold']}\")\n",
    "    print(\n",
    "        \"Accuracy: {:.3f}, Recall: {:.3f}, Precision: {:.3f}, F1: {:.3f}\".format(\n",
    "            res[\"accuracy\"], res[\"recall\"], res[\"precision\"], res[\"f1\"]\n",
    "        )\n",
    "    )\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(res[\"confusion_matrix\"])\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
